/* Image structure */
struct image {
  buffer : Buffer,      /* Buffer that contains host data */
  device_data : Buffer, /* Data in the target device */
  shm_data : Buffer,    /* Data in the target device shared memory */
  width : i32,          /* Image width */
  height : i32          /* Image height */
}

/* Filter structure */
struct filter {
  buffer : Buffer,      /* Buffer that contains target device data */
  width: i32,           /* Filter width */
  height: i32           /* Filter height */
}

/* Iterate from min to max executing a function */
fn range(mut min : i32, max : i32, body : fn(i32) -> ()) -> () {
  /* While maximum value not reached */
  while min < max {
    /* Call the given function */
    body(min);
    /* Go to next iteration */
    min++;
  }
}

/* Iterate from min to max with a offset executing a function */
fn offset_range(mut min : i32, max : i32, offset : i32, body : fn(i32) -> ()) -> () {
  /* While maximum value not reached */
  while min < max {
    /* Call the given function */
    body(min);
    /* Go to next iteration */
    min += offset;
  }
}

/* Exponential function */
fn exp(a : f64) -> f64 {
  /* Call GPU intrinsic exponential function provided by the AnyDSL runtime library */
  cuda_intrinsics.exp(a)
}

/* Write f64 (double) data to a buffer, considering it as an array of doubles */
fn write_f64(buffer : Buffer, index : i32, value : f64) -> () {
  bitcast[&mut[1][f64]](buffer.data)(index) = value
}

/* Read f64 (double) from a buffer, considering it as an array of doubles */
fn read_f64(buffer : Buffer, index : i32) -> f64 {
  bitcast[&[1][f64]](buffer.data)(index)
}

/* Sets image pixel at (x, y) coordinates */
fn set_pixel(img : image, x : i32, y : i32, value : f64) -> () {
  bitcast[&mut[1][f64]](img.device_data.data)(y * img.width + x) = value
}

/* Gets image pixel at (x, y) coordinates */
fn get_pixel(img : image, x : i32, y : i32) -> f64 {
  bitcast[&[1][f64]](img.device_data.data)(y * img.width + x)
}

/* Gets 1D filter coefficient at i index */
fn get_1d_filter_coeff(filt : filter, i : i32) -> f64 {
  bitcast[&[1][f64]](filt.buffer.data)(i)
}

/* Gets 2D filter coefficient at (i, j) index */
fn get_2d_filter_coeff(filt : filter, i : i32, j : i32) -> f64 {
  bitcast[&[1][f64]](filt.buffer.data)(j * filt.width + i)
}

/* Allocate memory in target device */
fn alloc_target(size : i32) -> Buffer {
  /* Use first CUDA accelerator */
  let accelerator = cuda_accelerator(0);

  /* GPU allocation */
  accelerator.alloc(size)
}

/* Get device data for image */
fn get_image_device_data(img : image) -> Buffer {
  /* Use first CUDA accelerator */
  let accelerator = cuda_accelerator(0);

  /* GPU allocation for the image */
  accelerator.alloc(img.width * img.height * sizeof[f64]())
}

/* Release image data from target device */
fn release_image(img : image) -> () {
  release(img.device_data);
}

/* Release filter data from target device */
fn release_filter(filt : filter) -> () {
  release(filt.buffer);
}

/* Specify input and output images where operations will be performed */
fn images(input_image : image, output_image : image, body : fn() -> ()) -> () {
  /* Use first CUDA accelerator */
  let accelerator = cuda_accelerator(0);
  /* Input image size in bytes */
  let image_size = input_image.width * input_image.height * sizeof[f64]();

  /* Copy input image content to GPU */
  copy(input_image.buffer, input_image.device_data, image_size);

  /* Execute the given operations */
  body();

  /* Copy output image GPU data to output host data */
  copy(output_image.device_data, output_image.buffer, image_size);
}

/* Iterates over an image in GPU */
fn iterate(input_image : image, output_image : image, body : fn(i32, i32) -> ()) -> () {
  /* Use first CUDA accelerator */
  let accelerator = cuda_accelerator(0);
  /* CUDA kernel grid configuration */
  let grid = (input_image.width, input_image.height, 1);
  /* CUDA kernel block configuration */
  let block = (128, 1, 1);

  /* Execute the following code in the GPU */
  with accelerator.exec(grid, block) @{
    /* Image x coordinate in the grid */
    let gid_x = accelerator.bidx() * accelerator.bdimx() + accelerator.tidx();
    /* Image y coordinate in the grid */
    let gid_y = accelerator.bidy() * accelerator.bdimy() + accelerator.tidy();

    /* Check for image boundaries to avoid invalid memory region access */
    if gid_x < input_image.width && gid_y < input_image.height {
      /* Call the body function with the obtained indexes */
      body(gid_x, gid_y);
    }
  }
}

/* Iterates over an image in GPU based on a stencil */
fn iterate_stencil(input_image : image, output_image : image, stencil : filter, body : fn(i32, i32) -> ()) -> () {
  /* Use first CUDA accelerator */
  let acc = cuda_accelerator(0);
  /* CUDA kernel grid configuration */
  let grid = (input_image.width, input_image.height, 1);
  /* CUDA kernel block configuration */
  let block = (128, 1, 1);

  /* Execute the following code in the GPU */
  with acc.exec(grid, block) @{
    /* Image x coordinate in the grid */
    let gid_x = acc.bidx() * acc.bdimx() + acc.tidx();
    /* Image y coordinate in the grid */
    let gid_y = acc.bidy() * acc.bdimy() + acc.tidy();
    /* Allocate shared memory for current block with size = (128 + 7 * 2) * (1 + 7 * 2) */
    let shared = reserve_shared[f64](2130);

    /* Copy input image content to shared memory */
    for j in offset_range(0, acc.bdimy() + stencil.height * 2, acc.bdimy()) {
      for i in offset_range(0, acc.bdimx() + stencil.width * 2, acc.bdimx()) {
        let img_index_x = gid_x - stencil.width + i;
        let img_index_y = gid_y - stencil.height + j;

        if img_index_x >= 0 && img_index_x < input_image.width && img_index_y >= 0 && img_index_y < input_image.height {
          shared((acc.tidy() + j) * (acc.bdimy() + stencil.height * 2) + acc.tidx() + i) = bitcast[&[1][f64]](input_image.buffer.data)(img_index_y * input_image.width + img_index_x);
        }
      }
    }

    /* Thread barrier synchronization */
    acc.barrier();

    /* Check for image boundaries to avoid invalid memory region access */
    if gid_x < input_image.width && gid_y < input_image.height {
      /* Call the body function with the obtained indexes */
      body(gid_x, gid_y);
    }
  }
}

/* Generates a 5x1 filter data structure given its coefficients */
fn filter_5x1(mask : Buffer, is_row : bool) -> filter {
  /* Use first CUDA accelerator */
  let accelerator = cuda_accelerator(0);
  /* Filter result */
  let mut result : filter;

  /* Allocates the filter data structure for a 5x1 column filter */
  if !is_row {
    result = filter {
      buffer: accelerator.alloc(5 * sizeof[f64]()),
      width: 5,
      height: 1
    };
  /* Allocates the filter data structure for a 5x1 row filter */
  } else {
    result = filter {
      buffer: accelerator.alloc(5 * sizeof[f64]()),
      width: 1,
      height: 5
    };
  }

  /* Copy filter data to GPU */
  copy(mask, result.buffer, result.width * result.height * sizeof[f64]());

  result
}

/* Generates a 5x5 filter data structure given its coefficients */
fn filter_5x5(mask : Buffer) -> filter {
  /* Use first CUDA accelerator */
  let accelerator = cuda_accelerator(0);

  /* Allocates the filter data structure for a 5x5 filter */
  let result = filter {
    buffer: accelerator.alloc(5 * 5 * sizeof[f64]()),
    width: 5,
    height: 5
  };

  /* Copy filter data to GPU */
  copy(mask, result.buffer, result.width * result.height * sizeof[f64]());

  result
}

/* Generates a 7x1 filter data structure given its coefficients */
fn filter_7x1(mask : Buffer, is_row : bool) -> filter {
  /* Use first CUDA accelerator */
  let accelerator = cuda_accelerator(0);
  /* Filter result */
  let mut result : filter;

  /* Allocates the filter data structure for a 7x1 column filter */
  if !is_row {
    result = filter {
      buffer: accelerator.alloc(7 * sizeof[f64]()),
      width: 7,
      height: 1
    };
  /* Allocates the filter data structure for a 7x1 row filter */
  } else {
    result = filter {
      buffer: accelerator.alloc(7 * sizeof[f64]()),
      width: 1,
      height: 7
    };
  }

  /* Copy filter data to GPU */
  copy(mask, result.buffer, result.width * result.height * sizeof[f64]());

  result
}

/* Generates a 7x7 filter data structure given its coefficients */
fn filter_7x7(mask : Buffer) -> filter {
  /* Use first CUDA accelerator */
  let accelerator = cuda_accelerator(0);

  /* Allocates the filter data structure for a 7x7 filter */
  let result = filter {
    buffer: accelerator.alloc(7 * 7 * sizeof[f64]()),
    width: 7,
    height: 7
  };

  /* Copy filter data to GPU */
  copy(mask, result.buffer, result.width * result.height * sizeof[f64]());

  result
}
