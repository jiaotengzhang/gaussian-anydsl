/* Image structure */
struct image {
  buffer : Buffer,      /* Buffer that contains host data */
  device_data : Buffer, /* Data in the target device */
  width : i32,          /* Image width */
  height : i32          /* Image height */
}

/* Filter structure */
struct filter {
  buffer : Buffer,      /* Buffer that contains target device data */
  size: i32             /* Filter size */
}

/* Iterate from min to max executing a function */
fn range(mut min : i32, max : i32, body : fn(i32) -> ()) -> () {
  /* While maximum value not reached */
  while min < max {
    /* Call the given function */
    body(min);
    /* Go to next iteration */
    min++;
  }
}

/* Exponential function */
fn exp(a : f64) -> f64 {
  /* Call GPU intrinsic exponential function provided by the AnyDSL runtime library */
  cuda_intrinsics.exp(a)
}

/* Write f64 (double) data to a buffer, considering it as an array of doubles */
fn write_f64(buffer : Buffer, index : i32, value : f64) -> () {
  bitcast[&mut[1][f64]](buffer.data)(index) = value
}

/* Read f64 (double) from a buffer, considering it as an array of doubles */
fn read_f64(buffer : Buffer, index : i32) -> f64 {
  bitcast[&[1][f64]](buffer.data)(index)
}

/* Sets image pixel at (x, y) coordinates */
fn set_pixel(img : image, x : i32, y : i32, value : f64) -> () {
  bitcast[&mut[1][f64]](img.device_data.data)(y * img.width + x) = value
}

/* Gets image pixel at (x, y) coordinates */
fn get_pixel(img : image, x : i32, y : i32) -> f64 {
  bitcast[&[1][f64]](img.device_data.data)(y * img.width + x)
}

/* Gets 1D filter coefficient at i index */
fn get_1d_filter_coeff(filt : filter, i : i32) -> f64 {
  bitcast[&[1][f64]](filt.buffer.data)(i)
}

/* Gets 2D filter coefficient at (i, j) index */
fn get_2d_filter_coeff(filt : filter, i : i32, j : i32) -> f64 {
  bitcast[&[1][f64]](filt.buffer.data)(j * filt.size + i)
}

/* Allocate memory in target device */
fn alloc_target(size : i32) -> Buffer {
  /* Use first CUDA accelerator */
  let accelerator = cuda_accelerator(0);

  /* GPU allocation */
  accelerator.alloc(size)
}

/* Get device data for image */
fn get_image_device_data(img : image) -> Buffer {
  /* Use first CUDA accelerator */
  let accelerator = cuda_accelerator(0);

  /* GPU allocation for the image */
  accelerator.alloc(img.width * img.height * sizeof[f64]())
}

/* Release image data from target device */
fn release_image(img : image) -> () {
  release(img.device_data);
}

/* Release filter data from target device */
fn release_filter(filt : filter) -> () {
  release(filt.buffer);
}

/* Specify input and output images where operations will be performed */
fn images(input_image : image, output_image : image, body: fn() -> ()) -> () {
  /* Use first CUDA accelerator */
  let accelerator = cuda_accelerator(0);
  /* Input image size in bytes */
  let image_size = input_image.width * input_image.height * sizeof[f64]();

  /* Copy input image content to GPU */
  copy(input_image.buffer, input_image.device_data, image_size);

  /* Execute the given operations */
  body();

  /* Copy output image GPU data to output host data */
  copy(output_image.device_data, output_image.buffer, image_size);
}

/* Iterates over an image in GPU */
fn iterate(input_image : image, output_image : image, body: fn(i32, i32) -> ()) -> () {
  /* Use first CUDA accelerator */
  let accelerator = cuda_accelerator(0);
  /* CUDA kernel grid configuration */
  let grid = (input_image.width, input_image.height, 1);
  /* CUDA kernel block configuration */
  let block = (128, 1, 1);

  /* Execute the following code in the GPU */
  with accelerator.exec(grid, block) @{
    /* Image x coordinate in the grid */
    let gid_x = accelerator.bidx() * accelerator.bdimx() + accelerator.tidx();
    /* Image y coordinate in the grid */
    let gid_y = accelerator.bidy() * accelerator.bdimy() + accelerator.tidy();

    /* Check for image boundaries to avoid invalid memory region access */
    if gid_x < input_image.width && gid_y < input_image.height {
      /* Call the body function with the obtained indexes */
      body(gid_x, gid_y);
    }
  }
}

/* Generates a 5x1 filter data structure given its coefficients */
fn filter_5x1(mask : Buffer) -> filter {
  /* Use first CUDA accelerator */
  let accelerator = cuda_accelerator(0);

  /* Allocates the filter data structure for a 5x1 filter */
  let result = filter {
    buffer: accelerator.alloc(5 * sizeof[f64]()),
    size: 5
  };

  /* Copy filter data to GPU */
  copy(mask, result.buffer, result.size * sizeof[f64]());

  result
}

/* Generates a 5x5 filter data structure given its coefficients */
fn filter_5x5(mask : Buffer) -> filter {
  /* Use first CUDA accelerator */
  let accelerator = cuda_accelerator(0);

  /* Allocates the filter data structure for a 5x5 filter */
  let result = filter {
    buffer: accelerator.alloc(5 * 5 * sizeof[f64]()),
    size: 5
  };

  /* Copy filter data to GPU */
  copy(mask, result.buffer, result.size * result.size * sizeof[f64]());

  result
}

/* Generates a 7x1 filter data structure given its coefficients */
fn filter_7x1(mask : Buffer) -> filter {
  /* Use first CUDA accelerator */
  let accelerator = cuda_accelerator(0);

  /* Allocates the filter data structure for a 7x1 filter */
  let result = filter {
    buffer: accelerator.alloc(7 * sizeof[f64]()),
    size: 7
  };

  /* Copy filter data to GPU */
  copy(mask, result.buffer, result.size * sizeof[f64]());

  result
}

/* Generates a 7x7 filter data structure given its coefficients */
fn filter_7x7(mask : Buffer) -> filter {
  /* Use first CUDA accelerator */
  let accelerator = cuda_accelerator(0);

  /* Allocates the filter data structure for a 7x7 filter */
  let result = filter {
    buffer: accelerator.alloc(7 * 7 * sizeof[f64]()),
    size: 7
  };

  /* Copy filter data to GPU */
  copy(mask, result.buffer, result.size * result.size * sizeof[f64]());

  result
}
